{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy.random import rand, randint, exponential\n",
    "\n",
    "from src.environment.network_rm import network_rm\n",
    "from src.agent.dqn.dqn import exp_replay\n",
    "from src.agent.dqn.models  import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inventory = np.array([6,6,6,20]) # the last item is time\n",
    "demand_types = np.array([[0,0,0,0], [1,1,1,0], [2,0,0,0]])\n",
    "demand_values = np.array([0,1,20])\n",
    "demand_arrivals = np.array([0.5, 0.5])\n",
    "inter_arrival_time = 1\n",
    "g = network_rm(inventory, demand_values, demand_types, demand_arrivals, inter_arrival_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Brain maps from observation to Q values for different actions.\n",
    "# Here it is a done using a multi layer perceptron with 2 hidden\n",
    "# layers\n",
    "\n",
    "#observation_shape has to be a 1d vector for now\n",
    "brain = MLP(list(g.observation_shape), [50, 50, g.num_actions], # change size to larger observation arrays\n",
    "            [tf.tanh, tf.tanh, tf.identity])\n",
    "\n",
    "# The optimizer to use. Here we use RMSProp as recommended\n",
    "# by the publication\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.001, decay=0.9)\n",
    "\n",
    "# DiscreteDeepQ object\n",
    "current_controller = exp_replay(g.observation_shape, g.num_actions, brain, optimizer, session,\n",
    "                                   discount_rate=0.99, exploration_period=1000, max_experience=100, \n",
    "                                   store_every_nth=4, train_every_nth=4)\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "session.run(current_controller.target_network_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_sim(env, agent, steps = 100, disable_training = False):\n",
    "    last_observation = None\n",
    "    last_action      = None\n",
    "    \n",
    "    for s in range(steps):\n",
    "        if env.terminate:\n",
    "            break\n",
    "        new_observation = env.observe()\n",
    "        reward          = env.collect_reward()\n",
    "        #print(g.total_reward, \" \", g.last_reward)\n",
    "        \n",
    "        # store last transition\n",
    "        if last_observation is not None:\n",
    "            agent.store(last_observation, last_action, reward, new_observation)\n",
    "\n",
    "        # act\n",
    "        new_action = agent.action(new_observation)\n",
    "        env.perform_action(new_action)\n",
    "    \n",
    "        #transition\n",
    "        env.transition()\n",
    "        \n",
    "        #train\n",
    "        if not disable_training:\n",
    "            agent.training_step()\n",
    "\n",
    "        # update current state as last state.\n",
    "        last_action = new_action\n",
    "        last_observation = new_observation\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One instance of network rm problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = network_rm(inventory, demand_values, demand_types, demand_arrivals, inter_arrival_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g.print_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = g.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 100\n",
    "%prun T = run_sim(g, current_controller, T)\n",
    "print(\"average reward: {}\".format((g.total_reward - x)/T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = 1000\n",
    "rewards = np.zeros(S)\n",
    "for i in range(S):\n",
    "    g = network_rm(inventory, demand_values, demand_types, demand_arrivals, inter_arrival_time)\n",
    "    T = run_sim(g, current_controller, 100)\n",
    "    rewards[i] = g.total_reward\n",
    "    if i%(S/20) == 0 and i >= (S/20):\n",
    "        print(\"average reward: {}\".format(rewards[(i-int(S/20)):i].mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
